output_dir = '/data/diffusion_pipe_training_runs/tmp'
dataset = '/home/mluser/workdir/video_generation/wan2.1_i2v_lora_training_config/dataset.toml'

# training settings
epochs = 50
micro_batch_size_per_gpu = 2
pipeline_stages = 1
gradient_accumulation_steps = 4
gradient_clipping = 1
warmup_steps = 10

# eval settings
eval_every_n_epochs = 1
eval_before_first_step = true
eval_micro_batch_size_per_gpu = 1
eval_gradient_accumulation_steps = 1

# misc settings
save_every_n_epochs = 2
checkpoint_every_n_minutes = 60
activation_checkpointing = 'unsloth'
partition_method = 'parameters'
save_dtype = 'bfloat16'
caching_batch_size = 8
map_num_proc = 22
steps_per_print = 1
video_clip_mode = "single_middle"
blocks_to_swap = 32

[model]
type = 'wan'
ckpt_path = '/data/generation_experiments/models/Wan2.1-I2V-14B-480P'
dtype = 'bfloat16'
transformer_dtype = 'float8'
timestep_sample_method = 'logit_normal'

[adapter]
type = 'lora'
rank = 32
dtype = 'bfloat16'

[optimizer]
type = 'AdamW8bitKahan'
lr = 5e-5
betas = [0.9, 0.99]
weight_decay = 0.01
stabilize = false


[monitoring]
enable_wandb = true
wandb_api_key = ''
wandb_tracker_name = 'wan2.1_i2v_14b_480p_lora_training'
wandb_run_name = ''